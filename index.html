<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding</title>
  <script type="text/javascript" src="assets/latexit.js"></script>
  <script type="text/javascript">
    LatexIT.add('p', true);
  </script>

  <!-- CSS includes -->
  <link href="assets/bootstrap.css" rel="stylesheet">
  <link href="assets/css.css" rel="stylesheet" type="text/css">
  <link href="assets/mystyle.css" rel="stylesheet">
  <link href="assets/lightbox2-2.11.3/dist/css/lightbox.css" rel="stylesheet" />

</head>

<body>

  <div id="header" class="container-fluid">
    <div class="row">
      <h1>OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding</h1>
      <div class="authors">
        Zong-Lin Gao, Sang NguyenQuang, Wen-Hsiao Peng, Xiem Hoang Van
      </div>
      <div class="conference">
        ICIP 2024
        <span style="display:inline-block; width: 5px;"></span>
        <a href="https://arxiv.org/abs/2402.12816" target="_blank"><img src="assets/icon/arxiv.jpg" height="35"></a>
        <!-- <span style="display:inline-block; width: 1px;"></span> -->
        <a href="https://github.com/NYCU-MAPL" target="_blank"><img src="assets/icon/GitHub-Mark.png" height="45"></a>
      </div>
    </div>

    <p style="text-align:center;">
      <a href="https://en.nycu.edu.tw/" target="_blank"><img src="assets/icon/210204-NYCU (1).png" height="90"></a>
      â€ƒ
      <a href="https://mapl.cs.nycu.edu.tw/content/" target="_blank"><img src="assets/icon/mapl_logo.png" height="120"></a>
    </p>
  </div>
  <div class="container" id="abstractdiv">
    <h2>Abstract</h2>
    <p>
Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency. However, the domain shift between training and test scenarios due to dataset limitations poses a challenge. This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs. Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance. To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method. It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec. Our OMRA is an online, inference technique. It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction. Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets. 
    </p>
    <h3><b>Domain Shift Issue</b></h3>
    <p style="text-align:center;">
      <a href="assets/DoaminShift.png" data-lightbox="arch"><img src="assets/DoaminShift.png" data-lightbox="arch"
          width="75%"></a> 
    </p>
    <p>
      The figure illustrates of the hierarchical B-frame prediction structures: (a) a 5-frame GOP for training and (b) a 32-frame GOP for test.  
    </p>
    <p>
      Domain shift issue arises from training learned B-frame codecs on short video sequences with a small group of pictures (GOP) but putting them to use on a large GOP. Most learned codecs, B-frame or P-frame, are trained on Vimeo-90k~\cite{vimeo}, where each training video is only 7-frame long.  The motion estimation network trained and optimized for small GOPs has difficulties in handling large motion often present in large GOPs.
    </p>
  </div>

  <div class="container" id="banner">
    <h2>Method</h2>
    <p style="text-align:center;">
      <a href="assets/OverView.png" data-lightbox="arch"><img src="assets/OverView.png" data-lightbox="arch"
          width="85%"></a>
    </p>
    <br>
    <p>
      The figure illustrates our proposed method. Encoding a B-frame $x_t$ begins by selecting one of the downsampling factors $S$ from {1, 2, 4, 8}. The motion estimation network (MENet) is then used to estimate two low-resolution optical flow maps $m^{(S)}_{t\to{t}-k}, m^{(S)}_{t\to{t}+k} \in \mathbb{R}^{2 \times \frac{W}{S} \times \frac{H}{S}}$ for $x^{(S)}_t$ with respect to $\hat{x}^{(S)}_{t-k}$ and $\hat{x}^{(S)}_{t+k}$. These flow maps are then collectively compressed by the conditional motion codec $(M^{enc}, M^{dec})$. In other words, the low-resolution flow maps are encoded into the bitstream. For motion compensation, the reconstructed flow maps $\hat{m}^{(S)}_{t\to{t}-k}, \hat{m}^{(S)}_{t\to{t}+k} \in \mathbb{R}^{2 \times \frac{W}{S} \times \frac{H}{S}}$ are upsampled to the original resolution before being fed into the frame synthesis network (FSNet). The FSNet performs multi-scale motion compensation in both the feature and pixel domains, producing the temporal predictor $mc^{(S)}_t \in \mathbb{R}^{3 \times W \times H}$, which serves as the conditioning signal for the inter-frame codec $(G^{enc}, G^{dec})$ to encode $x_t$.
    </p>
    </div>
      
    <div class="container" id="paperdiv">
      <h2>Paper</h2>
      <a href="assets/paper.pdf"
        download="TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception.pdf">
        <div class="thumbs">
      <img src="assets/ICIP2024/ICIP2024-1.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-2.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-3.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-4.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-5.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-6.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-7.png" width="19%">
    </div>
    </a>

    <div class="container" id="exp_results">
      <h2>Rate-distortion Results</h2>
      <p>
         The rate-accuracy plots for the competing methods. The methods are evaluated on three machine tasks: classification, object detection, and instance segmentation. For classification, we use ImageNet-val
         as the test set and a pre-trained ResNet50 as the downstream recognition network. For object detection and instance segmentation, we test the competing methods on COCO2017-val using a pre-trained Faster R-CNN and Mask R-CNN as the downstream recognition networks, respectively.
      </p>

      <div id="exp1">
        <div>
          <p>
            <a href="assets/RD/RD_HEVC-B.png" data-lightbox="RD Curve HEVC-B"><img
                src="assets/RD/RD_HEVC-B.png" width="360"></a>
            <a href="assets/RD/RD_UVG.png" data-lightbox="RD Curve UVG"><img
              src="assets/RD/RD_UVG.png" width="360"></a>
            <a href="assets/RD/RD_MCL-JCV.png" data-lightbox="RD Curve MCL"><img
              src="assets/RD/RD_MCL-JCV.png" width="360"></a>
          </p>
        </div>
      </div>
    </div>
<!-- 
    <div class="container" id="Visualization">
      <h2>Perframe Profiles</h2>
          <p style="text-align:center;">
            <a href="assets/PerFrame.png" data-lightbox="PerFrame"><img
                src="assets/PerFrame.png" width="700"></a>
          </p>
    </div> -->

    <div class="container" id="Visualization">

      <h2>Subjective Quality Comparison</h2>
      <p>
        Visualization of the warped frames and temporal predictors in BasketballDrive. The first row is input frames, with (a) (c) being the reference frames and (b) the target frame. The second row is results with OMRA, where (d) and (f) are temporally warped frames from (a) and (c), respectively. (e) is the temporal predictor. Likewise, the third row is the corresponding results without OMRA. $W(x_{t'},F_{t \to t'})$ denotes backward warping, where $x_{t'}$ is the reference frame to be warped and $F_{t \to t'}$ describes the backward flow from $x_t$ to $x_{t'}$.
      </p>
          <p style="text-align:center;"><b>HoneyBee</b></p>
            <p style="text-align:center;">

            <a href="assets/Visualization/HoneyBee.png" data-lightbox="HoneyBee"><img
                src="assets/Visualization/HoneyBee.png" width="540"></a>
            </p>

          <br>

          <p style="text-align:center;"><b>BasketballDrive</b></p>
          <p style="text-align:center;">
            <a href="assets/Visualization/BasketballDrive.png" data-lightbox="BasketballDrive"><img
                src="assets/Visualization/BasketballDrive.png" width="540"></a>
          </p>
    </div>

    <div id=footer><br></div>
    <!-- Javascript includes -->
    <script src="assets/jquery-1.js"></script>
    <script src="assets/bootstrap.js"></script>
    <script src="assets/lightbox2-2.11.3/dist/js/lightbox.js"></script>


</body>

</html>