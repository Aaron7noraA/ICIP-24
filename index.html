<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding</title>
  <script type="text/javascript" src="assets/latexit.js"></script>
  <script type="text/javascript">
    LatexIT.add('p', true);
  </script>

  <!-- CSS includes -->
  <link href="assets/bootstrap.css" rel="stylesheet">
  <link href="assets/css.css" rel="stylesheet" type="text/css">
  <link href="assets/mystyle.css" rel="stylesheet">
  <link href="assets/lightbox2-2.11.3/dist/css/lightbox.css" rel="stylesheet" />

</head>

<body>

  <div id="header" class="container-fluid">
    <div class="row">
      <h1>OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding</h1>
      <div class="authors">
        Zong-Lin Gao, Sang NguyenQuang, Wen-Hsiao Peng, Xiem Hoang Van
      </div>
      <div class="conference">
        ICIP 2024
        <span style="display:inline-block; width: 5px;"></span>
        <a href="https://arxiv.org/abs/2402.12816" target="_blank"><img src="assets/icon/arxiv.jpg" height="35"></a>
        <!-- <span style="display:inline-block; width: 1px;"></span> -->
        <a href="https://github.com/NYCU-MAPL" target="_blank"><img src="assets/icon/GitHub-Mark.png" height="45"></a>
      </div>
    </div>

    <p style="text-align:center;">
      <a href="https://en.nycu.edu.tw/" target="_blank"><img src="assets/icon/210204-NYCU (1).png" height="90"></a>
      â€ƒ
      <a href="http://mapl.nctu.edu.tw/" target="_blank"><img src="assets/icon/mapl_logo.png" height="120"></a>
    </p>
  </div>
  <div class="container" id="abstractdiv">
    <h2>Abstract</h2>
    <p>
Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency. However, the domain shift between training and test scenarios due to dataset limitations poses a challenge. This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs. Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance. To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method. It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec. Our OMRA is an online, inference technique. It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction. Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets. 
    </p>
  </div>

  <div class="container" id="banner">
    <h2>Method</h2>
    <p style="text-align:center;">
      <a href="assets/OverView.png" data-lightbox="arch"><img src="assets/OverView.png" data-lightbox="arch"
          width="85%"></a>
    </p>
    <br>
    <p>
      The figure illustrates our proposed method. Encoding a B-frame $x_t$ begins by selecting one of the downsampling factors $S$ from \{1, 2, 4, 8\}. The motion estimation network (MENet) is then used to estimate two low-resolution optical flow maps $m^{(S)}_{t\to{t}-k}, m^{(S)}_{t\to{t}+k} \in \mathbb{R}^{2 \times \frac{W}{S} \times \frac{H}{S}}$ for $x^{(S)}_t$ with respect to $\hat{x}^{(S)}_{t-k}$ and $\hat{x}^{(S)}_{t+k}$. These flow maps are then collectively compressed by the conditional motion codec ($M^{enc}, M^{dec}$). In other words, the low-resolution flow maps are encoded into the bitstream. For motion compensation, the reconstructed flow maps $\hat{m}^{(S)}_{t\to{t}-k}, \hat{m}^{(S)}_{t\to{t}+k} \in \mathbb{R}^{2 \times \frac{W}{S} \times \frac{H}{S}}$ are upsampled to the original resolution before being fed into the frame synthesis network (FSNet). The FSNet performs multi-scale motion compensation in both the feature and pixel domains, producing the temporal predictor $mc^{(S)}_t \in \mathbb{R}^{3 \times W \times H}$, which serves as the conditioning signal for the inter-frame codec ($G^{enc}, G^{dec}$) to encode $x_t$.
    </p>
    </div>
      
    <div class="container" id="paperdiv">
      <h2>Paper</h2>
      <a href="assets/paper.pdf"
        download="TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception.pdf">
        <div class="thumbs">
      <img src="assets/ICIP2024/ICIP2024-1.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-2.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-3.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-4.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-5.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-6.png" width="19%">
      <img src="assets/ICIP2024/ICIP2024-7.png" width="19%">
    </div>
    </a>

    <div class="container" id="exp_results">
      <h2>Rate-distortion Results</h2>
      <p>
        The rate-accuracy plots for the competing methods. The methods are evaluated on three machine tasks: classification, object detection, and instance segmentation. For classification, we use ImageNet-val
         as the test set and a pre-trained ResNet50 as the downstream recognition network. For object detection and instance segmentation, we test the competing methods on COCO2017-val using a pre-trained Faster R-CNN and Mask R-CNN as the downstream recognition networks, respectively.
      </p>

      <div id="exp1">
        <div class="col-md-6 vcenter">
          <p style="text-align:center;">
            <a href="assets/RD/RD_HEVC-B.png" data-lightbox="RD Curve HEVC-B"><img
                src="assets/RD/RD_HEVC-B.png" width="1080"></a>
            <a href="assets/RD/RD_UVG.png" data-lightbox="RD Curve UVG"><img
              src="assets/RD/RD_UVG.png" width="1080"></a>
            <a href="assets/RD/RD_MCL-JCV.png" data-lightbox="RD Curve MCL"><img
              src="assets/RD/RD_MCL-JCV.png" width="1080"></a>
          </p>
        </div>
      </div>
      <div class="col-md-12">
        <hr>
      </div>
      <p>
        We also compare our TransTIC with the methods recently submitted to the call-for-proposals (CFP) 
        competition of the MPEG VCM standard based on their test protocol. 
        The results of these competing methods are from
        the CFP test report (m61010). As shown in Fig. B1, our
        TransTIC performs comparably to the top performers in
        terms of rate-accuracy performance. However, our base
        codec has the additional constraint that it is optimized for
        human perception, while the top performers (e.g. p12, p6,
        p7) optimize the entire codec end-to-end for machine tasks.
        This shows the potential of our TransTIC.
      </p>

      <div id="exp2">
        <div class="row">
          <!-- <p style="text-align:center;">
            <a href="assets/results/rd2SOTA.png" data-lightbox="Exp2"><img
                src="assets/results/rd2SOTA.png" width="1080"></a>
          </p> -->
          <p style="text-align:center;">
            <a href="assets/results/rd2SOTA_detection.png" data-lightbox="Exp2"><img
                src="assets/results/rd2SOTA_detection.png" width="480"></a>
          <!-- </p> -->
          <!-- <p style="text-align:center;"> -->
            <a href="assets/results/rd2SOTA_segmentation.png" data-lightbox="Exp2"><img
                src="assets/results/rd2SOTA_segmentation.png" width="480"></a>
          </p>
        </div>
      </div>
    </div>

    <div class="container" id="Visualization">
      <h2>Qualitative Comparison</h2>
      <p>
        Decoded images and the bit allocation maps produced by the competing methods. As shown, $TIC$, the codec optimized for human perception,
         tends to allocate more bits to complex regions, even if those regions are less relevant (e.g. background) to the downstream recognition tasks. 
         In contrast, the other methods, which target machine perception, attempt to shift coding bits from the background regions to the foreground objects.
      </p>
      <!-- <div class="col-md-6 text-center">
        <p style="text-align:center;">
        <h3>Classification</h3>
        </p>
      </div>
      <div class="col-md-6 text-center">
        <p style="text-align:center;">
        <h3></h3>
        </p>
      </div> -->

      <div id="exp1">
        <div class="col-md-6 vcenter">
          <p style="text-align:center;">
            <a href="assets/results/dog.png" data-lightbox="Exp1"><img
              src="assets/results/dog.png" width="1080"></a>
              <a href="assets/results/dog2.png" data-lightbox="Exp1"><img
                src="assets/results/dog2.png" width="1080"></a>
            <a href="assets/results/bird.png" data-lightbox="Exp1"><img
                src="assets/results/bird.png" width="1080"></a>
          </p>
        </div>
      </div>
      <div class="col-md-12">
        <hr>
      </div>
      <div id="exp2">
        <div class="col-md-6 vcenter">
          <p style="text-align:center;">
            <a href="assets/results/ski.png" data-lightbox="Exp2"><img
              src="assets/results/ski.png" width="1080"></a>
            <a href="assets/results/kids.png" data-lightbox="Exp2"><img
              src="assets/results/kids.png" width="1080"></a>
            <a href="assets/results/baseball.png" data-lightbox="Exp2"><img
                src="assets/results/baseball.png" width="1080"></a>
          </p>
        </div>
      </div>
      <div class="col-md-12">
        <hr>
      </div>
      <div id="exp3">
        <div class="col-md-6 vcenter">
          <p style="text-align:center;">
            <a href="assets/results/ski2.png" data-lightbox="Exp3"><img
              src="assets/results/ski2.png" width="1080"></a>
            <a href="assets/results/kids2.png" data-lightbox="Exp3"><img
              src="assets/results/kids2.png" width="1080"></a>
          </p>
        </div>
      </div>

    </div>

    <div id=footer><br></div>
    <!-- Javascript includes -->
    <script src="assets/jquery-1.js"></script>
    <script src="assets/bootstrap.js"></script>
    <script src="assets/lightbox2-2.11.3/dist/js/lightbox.js"></script>


</body>

</html>